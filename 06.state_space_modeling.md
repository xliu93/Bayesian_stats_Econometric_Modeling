## 06. State Space Modeling (Filtering and Smoothing)

### Warm-up
1. Reality:
	- little observation we have
	- there are hidden state in the time series data
2. State space model:
	- a unifying framework
	- e.g. classical models like ARMA, and vectorized versions (VAR)
	- many applications, like speech recognization 

### State Space Model

#### Model specification:
1. **Hidden state model**: an initial distribution specifies the **prior** probability distribution $p(x_0)$ of the hidden state $x_0$ at the initial time step $k=0$;
2. **State transition model**: the system dynamics, as a discrete-time Markov process (memoryless), defined in terms of the transition probability distribution - $p(x_k|x_{k-1})$
3. **Conditional measurement model**: describes how the measurement $y_k$ depends on the current state $x_k$, defined by the conditional probability distribution of the measurement given the state $p(y_k|x_k)$.

#### Definitions:
Given the observations $Y = \{y_1,y_2,...y_T\}$, and we denote the state variables as $X=\{x_0,x_1,...,x_T\}$:

1. Filtering distribution: $p(x_k|y_{\leq{k}}), k\in[1,T]$
	- the marginal distribution of the current state $x_k$ given the current and previous measurements
2. State prediction distribution: $p(x_{k+n}|y_{\leq{k}})$
3. Smoothing distribution: $p(x_k|Y)$
	- smoothing density continues to change as more data comes in.

#### Problem setup

1. State space model with unknown parameters:
	- $\theta\sim{p(\theta)}$
	- $x_0\sim{p(x_0|theta)}$
	- $x_k\sim{p(x_k|x_{k-1},\theta)}$
	- $y_k\sim{p(y_k|x_k,\theta)}$

2. old-fashioned bayesian method:
	- First, produces a sequence of distributions $p(y_k|y_{k-1},\theta)$
	- can form the marginal posterior of parameters as $p(\theta|Y)\propto p(\theta)\prod{p(y_k|y_{k-1},\theta)}$
	- With the Markov property, the joint prior distribution of the states $p(x_0,x_1,...,x_T)$ and the joint likelihood of the measurements are
		$$p(x_0,x_1,...,x_T) = p(x_0)\prod{p(x_t|x_{t-1})}$$
		$$p(Y|x_0,x_1,...,x_T) = \prod{p(y_t|x_t)}$$
	- so the posterior is $p(X|Y)\propto p(X)p(Y|X)$
3. graphical model (see notes) -- "summarizing"
3. Problem: not suitable for real-time applications

#### Bayesian filter algorithm [a recursive method]
1. Algorithm procedure
	- compute the predictive distribution of the states:
		$$p(x_k|y_{\leq k-1}) = \int{p(x_k|x_{k-1})p(x_{k-1}|y_{\leq{k-1}})}dx_{k-1}$$ -- Chapman Kolmogorov equation
	- compute the posterior distribution of $x_k$ given $y_k$:
		$$p(x_k|y_{\leq k}) = \frac{1}{Z_k}p(y_k|x_k)p(x_k|y_{\leq k-1})$$ where $Z_k$ is the normalization constant, given by the integral of the rest of the expression over $x_k$:
		- $p(x_k|y_{\leq k})=p(x_k|y_k, y_{\leq k-1}) = p(y_k|x_k, y_{\leq k-1})p(x_k|y_{\leq k-1})/p(y_k|y_{\leq k-1})$, then $$Z_k = p(y_k|y_{\leq k-1})=\int_{x}p(y_k|x_k, y_{\leq k-1})dx$$
	- when the dynamic and measurement models are linear Gaussian, we have the Bayesian filtering equation as the Kalman Filter:
		$$\begin{align*}p(x_k|x_{k-1})&\sim N(x_k|A_{k-1}x_{k-1}, Q_{k-1})\\ p(y_k|x_k)&\sim N(y_k|H_kx_k, R_k)\end{align*}$$ 
		- [Theorem 12.1] The Bayesian filtering equations for the linear filtering model can be evaluated in closed form, and the resulting distributions are Gaussian. 

3. Two lemmas [Important for the proof of Theorem 12.1]
	- lemma 1: *from conditional to joint distribution* -- If random variables $x\in\mathbb{R}^n$ and $y\in\mathbb{R}^n$ have the Gaussian probability distributions: $$x\sim N(m,P), y|x \sim N(Hx+u, R)$$ Then the joint distribution of $x,y$ and the marginal distribution of $y$ are given as $$\left(\array{x\\y}\right) \sim N\left(\left(\array{m\\Hm+u}\right), \left(\array{P & PH^T \\ HP & HPH^T+R}\right)\right)$$ $$y\sim N\left(Hm+u, HPH^T+R\right)$$
	- lemma 2: *given joint distribution, find the marginal and conditional distribution* -- If the random variables $x$ and $y$ have the joint Gaussian probability distribution $$\left(\array{x\\y}\right)\sim N\left(\left(\array{a\\b}\right), \left(\array{A&C\\C^T&B}\right)\right)$$ then the marginal and conditional distributions of x and y are given as $$x\sim N(a,A), y\sim N(b,B)$$ $$x|y\sim N(a+CB^{-1}(y-b), A-CB^{-1}C^T)$$ $$y|x\sim N(b+C^TA^{-1}(x-a), B-C^TAC)$$
4. Two steps:
	- update step: $$\begin{align*}m_k^- &= A_{k-1}m_{k-1}\\P_k^{-}&=A_{k-1}P_{k-1}A_{k-1}^T+Q_{k-1}\end{align*}$$
	- prediction step: $$\begin{align*}m_k&=m_k^-+K_{k}(y_k-H_km_k^-), \\ P_k&=P_k^- - K_k(H_kP_k^-H_k^T+R_k)K^T\\\text{where}~& K_k = P_k^-H_k^-S_k^{-1}\end{align*}$$
5. numerical stability concern: better to work with matrix square roots of covariances instead of plain covariance matrices -- covariance matrix: psd: all eigen values are non-negative. [See Grewal and Andrews (2001)]

### Smoothing problem
#### Problem setup
1. compute the best possible estimates for each state conditional on all the measurements observed. 
2. difference with filtering problem:
	- having the whole series 1:T -- smoothing 
	- only based on information 1:k -- filtering
 
#### Bayesian smoother
Also called the backward recursive equations, for computing the smoothed distributions $$p(x_k|y_{\leq T}),~ \forall k<T$$ are given by the following equations: $$p(x_k|y_{\leq T})=p(x_k|y_{\leq k})\int{\frac{p(x_{k+1}|x_k)p(x_{k+1}|y_{\leq T})}{p(x_k+1|y_{\leq k})}}dx_{x+1}$$

1. computing the smoothed distributions for k<T
2. then gives the filtering distribution at step k 

#### RTS smoother/Kalman smoother

-- focusing on Gaussian processes: only one that can be solved in closed form. Explicitly the smoothing recursion is:
$$\begin{align*}
m_k^s = m_k + G_k(m_{k+1}^s-m_{k+1}^-)\\
P_k^s = P_k + G_k(P_{k+1}^s - P_{k+1}^-)G_k^T 
\end{align*}$$
where $$G_k=P_kA_k^T(P_{k-1}^-)^{-1}$$
And the recursion starts from the last timestamp $T$, with $$m_T^s = m_T, P_T^s = P_T$$

Note that $m$ and $P$ come from filtering distributions, and $m^-, P^-$ comes from prediction distribution, and $m^s, P^s$ stand for smoothing distribution. 

#### Code example in Jave
```java
public class KalmanModel {
	public final DoubleMatrix A, H, Q, R; 
	private final DoubleMatrix At, Ht;
	private Gaussian prior;
	
	public KalmanModel(DoubleMatrix a, DoubleMatrix h, DoubleMatrix q,
						  DoubleMatrix r) {
		A = a;
		H = h;
		Q = q;
		R = r;
		At = A.transpose(); 
		Ht = H.transpose();
	} 
}

public StatePredDensity prediction(FilterDensity f) { 
	DoubleMatrix m_minus = A.mmul(f.mean()); 
	DoubleMatrix X = A.mmul(f.cov().mmul(At)); 
	X.addi(Q);
	return new StatePredDensity(new Gaussian(m_minus, X), f.index() + 1);
}

public KalmanTripleSeq filter(ObservationSeq ys) { 
	KalmanTripleSeq result = new KalmanTripleSeq(); 
	FilterDensity prev = new FilterDensity(prior, 0); 
	for (Observation y : ys) {
		StatePredDensity pred = prediction(prev); 
		KalmanTriple triple = update(pred, y); 
		result.add(triple);
		prev = triple.fd;
	}
	return result;
}

```

## 05. Solve Logistic Regression by Iteratively Weighted Regression

1. Review logistic regression:
	- $p(y=1|x,w) = \sigma(w^Tx)$, or with a bias parameter $$p(y=1|x,w) = \sigma(b+w^Tx)$
	- denote $\mu_i:=\mu(x_i)=\sigma(w^Tx_i)$:
		- negative log-likelihood for dataset ${x_i}$: $$f(w)=-\sum{\log{\left[\mu_i^{\mathbf{1}(y_i=1)}\times(1-\mu_i)^{\mathbf{1}()y_i=0}\right]}} \\=-\sum{\left[y_i\log{\mu_i}+(1-y_i)\log(1-\mu_i)\right]}$$
		- cannot have MLE in closed form

2. Optimization method to compute the minimizer of log-likelihood:
	- gradient:$$\pmb{g}=\nabla{f}(w)=\sum_i{(\mu_i-y_i)x_i}=X^T(\pmb{\mu-y})$$
	- Hessian:$$\pmb{H}=\nabla{g}(w)=\sum_i{\mu_i(1-\mu_i)x_ix_i^T}=X^TSX$$ where $S=\text{diag}$
	- __Apply Newton's algorithm__: $$\begin{align*}{\bf{w}}_{k+1} &= {\bf w}_k-\pmb{H}^{-1}\pmb{g}_k\\ &= {\bf w}_{k}+(X^TS_kX)^{-1}X^T({\bf y}-\pmb{\mu}_k)\\&=(X^TS_kX)^{-1}\left[(X^TX_kX){\bf w}_k+X^T({\bf y}-\pmb{\mu}_k)\right]\\&=(X^TS_kX)^{-1}X^T[S_kX{\bf w}_k+{\bf y}-\pmb{\mu}_k]\end{align*}$$
	- let's define the _working response_ as $${\bf z}_k = X{\bf w}_k+S_k^{-1}({\bf y}-\pmb{\mu}_k)$$ then we have $${\bf w}_{k+1}=(X^TS_kX)^{-1}X^TS_k{\bf z}_k$$ which can be regarded as the weighted least squares estimator for regression response $z_k$ on predictors $X$ with weights $S_k$.  -- this is the **Iteratively Weighted least squares** or **IRLS**: at each iteration, solve a weighted least squares problem and update the weight matrix (S_k) at each iteration. 
		- Benefit: $S_k$ is diagonal, so computation of the working response does not require matrix inversion.



## 04. Gradient Descent

### I. Descent Direction
1. Descent direction
2. Descent algorithm
	- an iterative procedure of the form $$\theta_{k+1} = \theta_k + \eta_k\pmb{d}_k$$
	- Gradient descent
		- main issue: step size 
			- constant: not a good idea
		- looking for a more stable method for picking the step-size -- convergence (to the local optimum) is guaranteed.

### II. Line Search
1. Picking the best $\eta_k$ -- 1-d optimization problem: 
	- minimize $\phi(\eta)$ where $$\phi(\eta)=f(\theta_k+\eta d_k)$$ and $d_k$ is a descent direction.
2. algorithms:
	- backtracking line search
	- More-Thuente line search
	- Exact line search (minimize the cost)
3. Examples and comparison
	- E.g.7.1 (zig-zag behavior)
	- cases when Exact line search outperfoms the others: no ``long narrow valley'' exists.

### III. Second order optimization methods
1. Derive __faster__ optimization methods by taking the curvature, i.e. the Hessian, into account: e.g. Newton's algorithm
	- iterative algorithm: updates of the form $$\theta_{k+1} = \theta_k-\eta_k\pmb{H}^{-1}\pmb{g}_k$$
2. Derivation of Newton's algorithm
	- Making a second-order Taylor series approximation of $f(\theta)$ around $\theta_k$ we have $$f_{quad}(\theta) = f_k+\pmb{g}_k^T(\theta-\theta_k)+\frac{1}{2}(\theta-\theta_k)^T\pmb{H}_k(\theta-\theta_k)$$
	- completing the square:$$f_{quad}(\theta)=\frac{1}{2}\theta^TA\theta + b^T\theta+c$$ where 
		- $A=\pmb{H}_k$, 
		- $b=\pmb{g}_k$, 
		- $c=f_k-\pmb{g}_k^T\theta_k+\frac{1}{2}\theta_k^T\pmb{H}_k\theta_k$
	- the minimum of $f_{quad}$ is at $\theta=-A^{-1}b = \theta_k-\pmb{H}_k^{-1}\pmb{g}_k$. Thus the Newton step $\pmb{d}_k = -\pmb{H}_k^{-1}\pmb{g}_k$ is what should be added to $\theta_k$ to minimize the second order approximation of $f$ around $\theta_k$.

3. More on Newton's algorithm:
	- as long as $\pmb{H}_k$ has no negative eigenvalues, i.e. psd, then the Newton step $\pmb{d}_k = -\pmb{H}_k^{-1}g_k$ is the descent direction.
	- positive definiteness of H will hold if the function is strictly convex
	- *Levenberg-Marquardt algorithm

### IV. Conjugate Gradient Descent
1. truncated Newton
	- Rather than computing $\pmb{d}_k = -\pmb{H}_k^{-1}\pmb{g}_k$ directly, we solve the linear system of $$\pmb{H}_k\pmb{d}_k=-\pmb{g}_k$$ for $\pmb{d}_k$ using an iterative method.
	- truncate the iterations as soon as negative curvature/Hessian is detected ($\pmb{H}_k$ is not positive definite anymore)
	- e.g. conjugate gradient descent
2. Conjugate vectors:  
	- Two vectors are said to be conjugate if and only if they are orthogonal w.r.t. the following inner product:$$\langle{v,y}\rangle_A:=\langle{Au,v}\rangle=\langle{u,A^Tv}\rangle=u^TAv=0$$ for any symmetric and positive definite matrix $A$. 
3. Conjugate Gradient descent algorithm
	- Suppose that $P=\{p_1,p_2,...,p_n\}$ a set of mutually conjugate vectors, which forms a basis for $\mathbb{R}^n$
	- the solution $x^*$ of $Ax=b$ in this basis is given by $$x^* = \sum_{i=1}^{n}\alpha_i p_i \Rightarrow \alpha_i = \frac{\langle{p_i, b}\rangle}{\langle{p_i,p_i}\rangle}_A$$ (similar to Gram-Schmidt process)
		- <mark>Take-away (or the main idea of Conjugate Gradient)</mark>: find a sequence of $n$ _conjugate directions_, and compute the coefficients $\alpha_k$. Then there exists an iterative method which produces, at the k-th step, a sequence $x_k$ which converges to $x^*$, and the k-th conjugate vector.
	- The solution $x^*$ is also the unique minimizer of the quadratic function $f(x)=\frac{1}{2}x^TAx - x^Tb, ~ x\in\mathbf{R}^n$, which suggests taking the first basis vector $p_0$ to be the negative of the gradient of $f$ at $x=x_0$.
	- __CG Procedure__:
		1. ) take $p_0 = b-Ax_0$
		2. ) at each $k$-th step, take the residual $r_k$ as the negative gradient of $f$ at $x=x_k$: $$r_k=b-Ax_k$$
		3. ) compute the conjugate gradient and the coefficient: $$p_k=r_k-\sum_{i<k}{\frac{\langle{p_i,r_k}\rangle_A}{\langle{p_i,p_i}\rangle_A}p_i}$$ $$\alpha_k = \frac{\langle{p_k, r_{k-1}}\rangle}{\langle{p_k,p_k}\rangle_A}=\frac{p_k^Tr_{k-1}}{p_k^TAp_k}$$
		4. ) following the direction of $p_k$, the update will be $$x_{k+1}=x_k+\alpha_k p_k$$
4. Properties:
	- Pro: fast and easy to implement
	- Con: ill-conditioned Hessian leads to slow convergence (the number of iterations for convergence is proportional to the condition number)
	- *_preconditioning method_: modify $A$ to reduce the condition number 

### V. Quasi-Newton Methods
1. Quasi-Newton Methods:
	- Motivation: expensive to compute $\pmb{H}$ Hessian; 
	- Instead, Quasi-Newton methods iteratively build up an approximation to the Hessian
	-  A common method: BFGS
		- rank-2 update to the matrix -- ensures that the matrix remains positive-definite
2. Space complexity: 
	- Hessian $O(D^2)$
	- limited-memory BFGS, aka L-BFGS: $O(mD)$
		- using only the $m$ most recent $(s_k,y_k)$ pairs and ignoring older information
	- Generalization of L-BFGS to handle _bound constraints_


## 01. Bayesian Thinking and Conjugate Prior

### Bayesian Thinking
* prior: $p(\theta)$ -- it summarizes the information that was known about the parameter before observation of the data presently being considered.
* likelihood: $p(x|\theta)$ distribution on the dataset
* posterior: $p(\theta|x)$

### Point estimate
* MLE -- maximum-likelihood estimation
  $$\hat\theta_{MLE} = argmax_{\theta}\left\{\log{p(x|\theta)}\right\}$$
* In Bayesian analysis, all inference is based around the posterior. The appropriate analog of the MLE is the maximum a-posterior estimator (MAP):
  $$\hat\theta_{MAP} = argmax_{\theta}\left\{\log{p(\theta|x)}\right\} = argmax_{\theta}\{\log{p(x|\theta)}+\log{p(\theta)}\}$$
  * the log-likelihood augmented by an additional penalty term: $\log{p(\theta)}$ -- a "regularizer".

### From Unfair-coin model to Conjugate Prior
* By assumption $\theta$ is uniform on [0,1], so $p(\theta)=1$, which gives
  * **likelihood:** $p(x|\theta)\propto\theta^{x}(1-\theta)^{n-x}$, considered as a function of $\theta$ gives the _beta distribution_: $Beta(x+1,n-x+1)$.
  * after observing x heads in n trials, what is the probability that the next flip, say $y$, is heads?
     * $p(y=1|x)=\int{p(y=1|\theta,x)p(\theta|x)d\theta} = \frac{x+1}{n+2}$, which is the posterior mean (mean of Beta($\alpha,\beta$) gives $\alpha/(\alpha+\beta)$)
     * "Laplace's Law of Succession"

* Now switching our prior to Beta($\alpha,\beta$)
  * **prior:** $p(\theta|\alpha,\beta)\propto\theta^{\alpha-1}(1-\theta)^{\beta-1}$
  * **posterior:** $p(\theta|x)\propto p(x|\theta)p(\theta)\backsim Beta(\alpha+x,\beta+n-x)$
  * Thus the posterior is of the same form as the prior, but with different parameters.
  * The beta distribution is said to be a **conjugate prior** for the binomial model.

* Conjugate prior is related to the likelihood functionï¼›
  - Generalize to the case in which each observation is one of $k$ possible outcomes:
     - $p(n_1,n_2,...,n_k|\theta)\propto\prod_{j=1}^{k}\theta_{j}^{n_j},~\theta_j>0,\sum{\theta_j}=1.$
     - conjugate prior: _Dirichlet distribution_, $p(\theta|x)\propto\prod\theta_{j}^{\alpha_{j}-1}$;
     - resulting posterior: Dirichlet with parameters $\alpha_j+n_j$

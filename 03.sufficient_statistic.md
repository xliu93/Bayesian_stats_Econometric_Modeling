## 03. Sufficient Statistic


### Motivation: 
- the information obtained from the observation of n real r.v.s can be condensed into a _low-dimensional vector of statistics_ which contains as much info about the parameter as do the whole observations.

### Define __sufficient statistic__
- __Definition__: A statistic for a sample of size $n$ is a map $$T:\mathcal{X}^n\mapsto\mathbb{R}^k$$ 
		- Let $R(T)$ denote the range of $T$, in the context of a parametric statistical model $p(x|\theta)$, the statistic $T$ is said to be sufficient for the parameter $\theta$ if for each $t\in R(T)$, the conditional distribution of $\{\mathbf{x}|T=t\} = (x_1,x_2,...,x_n)$ does not depend on $\theta$.
- mathematical expression: 
	- $p(X=x|T(X)=t,\theta) = \frac{p(X=x, T(X)=t|\theta)}{p(T(X)=t|\theta)}$ does not contain any $\theta$
- Interpretation: $\theta$ does not contain any additional info for _computing the likelihood function_ than what was covered by $T(x)=t$, or is to say any maximum likelihood estimates must be computable from $T(X)$.
- Examples: 
	- __E.g.1__: For Bernoulli distribution with parameter $\theta$, $T(X):=\sum_i{X_i} = t$ is a sufficient statistic, i.e., $p(X=x|T(X)=t,\theta)$ is independent of $\theta$. __(HW1.1)__
	- __E.g.2__: For independent draws from Poisson distribution with parameter $\theta$, $T(X):=\sum_i{X_i} = t$ is a sufficient statistic.
- What does it mean to carry all the sample information concerning the parameter?
	- a sample $\mathbb{x}$ with sufficient statistic $T(\mathbb{x}) = t$
	- we 'lost' the sample, but keep the value $t$
	- _can draw samples from $p(X|T(X)=t, \theta)$ to replace the one we lost._

### Likelihood ratio
- __Definition:__ In any parametric statistical model, the likelihood ratio is defined as $\Lambda_x(\theta_1,\theta_2)=\frac{p(x|\theta_1)}{p(x|\theta_2)}$
- (Lemma 6.1) For any function $t(x)$, the following are equivalent:
	- a). $p(x|\theta) = h(x)g(t(x),\theta)$ for some functions $g,h$
	- b). $t(x)=t(x') \Rightarrow \Lambda_x=\Lambda_{x'}$ 
	- [Proof: see note Page 4]
- _Fisher-Neyman factorization theorem_: (a) in lemma 6.1 is equivalent to sufficiency for t(x).

### Minimal sufficient statistic
- __Definition:__ A statistic $T$ is said to be minimal sufficient if for any sufficient statistic $U$, there exists a function $h$ s.t. $T=h(U)$.
- Properties: 
	- not unique: (Lemma 6.2) if $T$ and $S$ are minimal sufficient statistics, then $T=g(S), S=h(T)$
- Necessary and sufficient condition for minimal sufficient statistic: (Theorem 6.1) $$T(x) = T(x') \Leftrightarrow \Lambda_x=\Lambda_x'$$
- Practical meaning: represent the best possible data compression that still allows you to do arbitrary inference about the params.
	
### Complete statistic
- __Definition:__ A statistic $T$ is said to be complete w.r.t. $\mathcal{P}=\{p(\cdot|\theta)\}$ if $\mathbb{E}[f(T)] = 0 \Rightarrow f(T)=0$ almost everywhere w.r.t. $\mathcal{P}$
- REVIEW: _Iterated Expectation Formula_ -- $\mathbb{E}[X] = \mathbb{E}_Y[\mathbb{E}[X|Y=y]]$
- Relationship between sufficient + complete statistic and minimal sufficient: (Theorem 6.3)
	- Suppose $X$ has density $f(x;\theta)$ and $T(X)$ is sufficient and complete for $\theta$, then $T$ is minimal sufficient. 
	- Proof: see note P6
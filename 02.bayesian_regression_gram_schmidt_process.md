## 02. Regression

### Linear Models

1. Typical linear models:
	$$y | \beta, \sigma^2, X\backsim N(X\beta,\sigma^2I)$$
	
2. Bayesian methods:
	- likelihood: $p(y|\theta)$
	- log likelihood: $p(y|\theta)=\frac{n}{2}\log{(2\pi\sigma^2)} - \frac{1}{2\sigma^2}l(\beta)$ , where $l(\beta) = \lVert{y-X\beta}\rVert^{2}$
	- MLE: $\hat{\beta} = argmin_{\beta}{\mathit{l}(\beta)} = (X'X)^{-1}X'y = X^{+}y$
	- pseudoinverse: $X^{+}$

	__Definition__: The _Moore-Penrose_ pseudoinverse $X^{+}$ is defined so that $X^{+}y$ is the minimimum-norm vector among all minimizers of $l(\beta)$. 
	
3. A Slight generalization:
	$$y=X\beta+\varepsilon$$ where $Var[\varepsilon] = D:= diag(\sigma_1^2, \sigma_2^2, ..., \sigma_n^2)$ with all $\sigma_i^2>0$
	- The log-likelihood minimizer: $\hat{\beta} = \min_\beta{(y-X\beta)^TD^{-1}(y-X\beta)}$
	- $\hat{\beta} = (X'D^{-1}X)^{-1}X'D^{-1}y$ if $X'D^{-1}X$ is stably invertible
	- if $D=\sigma^2 I$, $\hat\beta$ can be regarded as the result of regressing $D^{-1/2}y$ on $D^{-1/2}X$.


### Bias and Variance of Estimators

1. Warm-up:
	- least square estimates of the parameter $\beta$ have the smallest variance among all linear unbiased estimates [__Gauss-Markov Theorem__]. (but some biased estimators might outperform)
	- <mark>biased estimators: ridge and lasso</mark>
	- bias-variance tradeoff (later)

2. Define bias and variance for an estimator

	- In a parametric statistical model with likelihood $p(x|\theta)$, an estimator is of form $x\mapsto\hat{\theta}(x)$ from a given dataset. The sampling distribution is the distribution on the data space $\mathcal{X}$ given by $p(x|\theta)$ if $\theta$ were known.  
	- Any estimator also has a sampling distribution: the distribution of $\hat\theta(x)$ where $x\sim{p(x|\theta)}$. 
	
	__Definition:__ bias and variance of the estimator: $$\text{bias}(\hat\theta):= \mathbb{E}[\hat\theta(x)-\theta], ~~\text{var}(\hat\theta):=\mathbb{V}[\hat\theta(x)-\theta]$$ where $\mathbb{E,V}$ are taken under the sampling distribution on the data space.

	- only theoretical, because the ground truth $\theta$ is unknown

3. Bias and variance of LSE (least-square estimator)
	- it is unbiased: for $\hat\theta=\alpha^T\hat\beta$, $\mathbb{E}[\hat\theta]=\alpha^T\beta$
	- also, if there exists another linear estimator $\widetilde{\theta}=c^Ty$, its variance is no less than the variance of LSE estimator: $\text{var}(\widetilde{\theta})\leq\text{var}(\alpha^T\hat{\beta})$, by Gauss-Markov Theorem.
	
4. Mean squared-error (MSE) of estimators:  
	For any estimator $\widetilde{\theta}$, not limited to the linear class, the mean squared-error is
	$$\text{MSE}(\widetilde{\theta}) = \mathbb{E}[(\theta-\widetilde{\theta})^2]=\text{var}(\widetilde\theta)+(\theta-\mathbb{E}[\widetilde\theta])^2$$
	- first term: variance of the estimator
	- second term: squared bias of the estimator.
	- therefore, Gauss-Markov implies that LSE gives the minimized mse estimation among all _unbiased_, _linear_ estimators.

### Multiple Regression
1. __Gram-Schmidt Procedure__  Given input matrix $X_{n\times p}$, and dependent variable $y$:

	- (1) Initialize $z_0=x_0=1$
	- (2) For $j$ = 1,2,...p, regress $x_j$ on $z_0, z_1,..., z_{j-1}$ to produce coefficients $\hat\gamma_{l,j} = \langle z_l, x_j\rangle/\langle z_l, z_l\rangle$ and residual vector $z_j = x_j-\sum_{k=0}^{j-1}\hat\gamma_{k,j}z_k$
	- (3) Regress $y$ on the residual $z_p$ to give the estimate $\hat\beta_p$.
	
	Gram-Schmit actually gives the same coefficient of $\hat\beta_p$ as the one from  regress $y$ on $X$. 

2. From Gram-Schmitdt procedure to QR decomposition
	- note that the original input matrix can be rewritten as $$X=Z\Gamma$$
		where $Z = [\array{z_0, z_1,...,z_p}]$, and $\Gamma$ is the upper triangle matrix with entries $\hat\gamma_{k,j}$
	- By introducing the diagonal matrix $D$ where $D_{j,j}=\lVert{z_j}\rVert$ we have $$X = ZD^{-1}D\Gamma = QR$$ where $Q = ZD^{-1}, R=D\Gamma$
	- the QR decomposition of X gives a convenient orthogonal basis for the column space of $X$. and the least square solution is given by $$\hat\beta = R^{-1}Q^Ty, \hat{y}=QQ^Ty$$ $\beta$ is easy to solve for $R$ is a upper triangle matrix.
	
### Instability
If $x_p$ is highly correlated with some of the other columns, the residual $z_p$ will be close to zero, and hence the coefficient $$\hat\beta_{p} = \frac{\langle z_p,y\rangle}{\langle{z_p,z_p}\rangle}$$ will be very unstable (large variance) see bel

- $\text{var}(\hat\beta_p) = \frac{var(\langle{z_p,y}\rangle)}{\langle{z_p,z_p}\rangle^2}= \frac{z_p^Tvar(\varepsilon)z_p}{\lVert{z_p}\rVert^4} = \frac{\sigma^2}{\lVert{z_p}\rVert^2}$ 

### Penalized Regression - Ridge and Lasso
(more on this topic from the DS-GA 1003 Machine Learning course by David Rosenberg.)  

---
Something I find useful to help me understand
(also for homework problems).  
1. [Gram-Schmidt procedure](https://mertricks.com/tag/gram-schmidt-process/)  
2. [Variance Inflation](http://www.stat.cmu.edu/~ryantibs/datamining/lectures/14-reg2-marked.pdf)  
3. [Hint](https://people.eecs.ku.edu/~jhuan/EECS940_S12/slides/linearRegression.pdf) for proof of Gram-Schmidt gives the same coefficient $\hat{\beta}_p$ as multiple regression. (expand the matrix expression)
